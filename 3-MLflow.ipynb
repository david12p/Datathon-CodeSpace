{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon III - MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid unnecessary warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First step always is data loading. CSV are provided and can be replicated from the previows lesson. CSVs contains data for the PRODUCT_ID 3960.\n",
    "\n",
    "- ðŸ‘‰ Load train and test datasets.\n",
    "- ðŸ‘‰ Set \"fecha_venta\" as index column.\n",
    "- ðŸ‘‰ Sort the data by \"fecha_venta\" column.\n",
    "- ðŸ‘‰ Show the first 5 rows of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/itemSalesTrain.csv\"                          #This file only includes data of the article 3960\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)             \n",
    "df_train = df_train.set_index(\"fecha_venta\").sort_index()            #Set fecha_venta as index and sort by fecha_venta column\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"data/itemSalesTest.csv\"\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "df_test = df_test.set_index(\"fecha_venta\").sort_index()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n",
    "\n",
    "We will split the data into train and test sets. We will use the train set to train the model and the test set to evaluate it.\n",
    "\n",
    "\n",
    "- ðŸ‘‰ Select the feature columns and the label columns\n",
    "- ðŸ‘‰ Split the data into train and validation sets using a 80/20 or 90/10 ratio.\n",
    "- ðŸ’¡ Remember how train/val split should be made in time series problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the product id and family (only for logging purposes, do not use it for filtering)\n",
    "PRODUCT_ID = 3960\n",
    "PRODUCT_FAMILY = \"BOLLERIA\"\n",
    "\n",
    "df_test.columns     # To see what dependent variable we want to use (var1(t)). Then, we set the label and the features\n",
    "\n",
    "LABEL_COLUMN = \"var1(t)\"\n",
    "\n",
    "FEATURES_COLUMN = [\n",
    "    'var1(t-7 day)',\n",
    "    'var1(t-6 day)', \n",
    "    'var1(t-5 day)', \n",
    "    'var1(t-4 day)',\n",
    "    'var1(t-3 day)', \n",
    "    'var1(t-2 day)', \n",
    "    'var1(t-1 day)', \n",
    "    'var1(t-5 week)',\n",
    "    'var1(t-4 week)', \n",
    "    'var1(t-3 week)', \n",
    "    'var1(t-2 week)', \n",
    "    'var1(t-1 week)',\n",
    "    'var1(t)', \n",
    "    'tavg_w', \n",
    "    'tmin_w', \n",
    "    'tmax_w', \n",
    "    'isfestivo', \n",
    "    'day',\n",
    "    'dayofweek', \n",
    "    'month', \n",
    "    'year', \n",
    "    'weekday'\n",
    "]\n",
    "\n",
    "features = df_train[FEATURES_COLUMN]\n",
    "labels = df_train[LABEL_COLUMN]\n",
    "\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "train_size = int(len(df_train) * (1 - VAL_SPLIT))       # 90% of the data\n",
    "\n",
    "X_train = features[:train_size]\n",
    "y_train = labels[:train_size]\n",
    "X_val = features[train_size:]\n",
    "y_val = labels[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start MLFlow Server\n",
    "\n",
    "- ðŸ‘‰ Launch a local MLFlow server\n",
    "- ðŸ‘‰ Connect to local MLFlow server\n",
    "- ðŸ‘‰ Set the desired experiment\n",
    "- ðŸ‘‰ Enable MLFlow autologing for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow==2.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the mlflow server hosted locally and setting the url in a variable\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)        #To launch the server, we open the terminal and write \"mlflow ui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we create a new experiment (I did it on the webpage, itÂ´s called \"Sales Forecasting\"), we have to connect to the experiment.\n",
    "MLFLOW_EXPERIMENT_NAME = \"Sales Forecasting\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)       # Set what the experiment is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the autlog sklearn\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "\n",
    "The next section is to train and evaluate the model. We will use a pipeline to preprocess the data and train the model.\n",
    "\n",
    "- ðŸ‘‰ Create a Sklearn Pipeline:\n",
    "  - ðŸ‘‰ Preprocessing: StandardScaler or MinMaxScaler\n",
    "  - ðŸ‘‰ Model: LinearRegression, RandomForestRegressor, etc.\n",
    "- ðŸ‘‰ Start a run in MLFlow\n",
    "- ðŸ‘‰ Train the model using the train dataset\n",
    "- ðŸ‘‰ Add convenient tags for PRODUCT_ID and FAMILY_ID\n",
    "- ðŸ‘‰ Evaluate the model\n",
    "- ðŸ’¡ Remember this is a regression problem\n",
    "- ðŸ’¡ Autolog will automatically log metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "model = LinearRegression()\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "with mlflow.start_run() as run:                             # Start a run with these information:\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    mlflow.set_tag(\"product_id\", PRODUCT_ID)                # Add tags in columns\n",
    "    mlflow.set_tag(\"product_family\", PRODUCT_FAMILY)\n",
    "\n",
    "    predictions = pipe.predict(X_val)\n",
    "    mae = metrics.mean_absolute_error(y_val, predictions)   # Add mae, mse and r2 metrics\n",
    "    mse = metrics.mean_squared_error(y_val, predictions)\n",
    "    r2 = metrics.r2_score(y_val, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "Promote the model to Model Registry. For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Sales Forecasting](data/model1_mlflow_production.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the Model\n",
    "\n",
    "We can assign a tag to the model to indicate that it is ready for production. This way all versions (v1, v2...) of the model will have the same tag. So we can deploy the model by selecting the (same) tag instead of a specific (different) version.\n",
    "\n",
    "For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Sales Forecasting Tag \"production\"](data/mlflow_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "In a terminal run the following command to deploy the model:\n",
    "\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "mlflow models serve -m models:/<model_name>@production -p 5001 --env-manager local\n",
    "```\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```bash\n",
    "[INFO] Starting gunicorn 21.2.0\n",
    "[INFO] Listening at: http://127.0.0.1:5001 (236041)\n",
    "[INFO] Using worker: sync\n",
    "[INFO] Booting worker with pid: 236048\n",
    "```\n",
    "\n",
    "It means it's working correctly ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make requests to the model\n",
    "\n",
    "The model is now deployed and ready to receive requests. We will make a request to the model using the test set.\n",
    "\n",
    "- ðŸ‘‰ prepare the test set to be sent as JSON\n",
    "- ðŸ‘‰ make a POST request to the model\n",
    "- ðŸ‘‰ get the predictions from the response and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "URL = \"http://localhost:5001/invocations\"\n",
    "#URL = \"http://localhost:5001/health\"            \n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": df_test.columns.tolist(),\n",
    "        \"index\": df_test.index.to_list(),\n",
    "        \"data\": df_test.values.tolist()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(URL)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(URL, headers=headers, data=json.dumps(data))\n",
    "predictions = response.json()[\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Results to Database\n",
    "\n",
    "We push the results to the database so we can visualize them using other tools like Tableau, PowerBI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpfull class used to connect to the database and push dataframes\n",
    "\n",
    "import sqlalchemy as sa\n",
    "\n",
    "class DatabaseConnection:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username: str,\n",
    "        password: str,\n",
    "        dialect: str = \"mysql\",\n",
    "        driver: str = \"pymysql\",\n",
    "        host: str = \"database-1.cxlpff3hacbu.eu-west-3.rds.amazonaws.com\",\n",
    "        port: int = 3306,\n",
    "        database: str = \"sandbox\",\n",
    "    ) -> None:\n",
    "        \"\"\"Creates a connection to a database\n",
    "\n",
    "        Args:\n",
    "            username (str): username\n",
    "            password (str): password\n",
    "            dialect (str, optional): dialect. Defaults to \"mysql\".\n",
    "            driver (str, optional): driver. Defaults to \"pymysql\".\n",
    "            host (str, optional): host. Defaults to \"database-1.crek3tiqyj7r.eu-west-3.rds.amazonaws.com\".\n",
    "            port (int, optional): port. Defaults to 3306.\n",
    "            database (str, optional): database. Defaults to \"classicmodels\".\n",
    "        \"\"\"\n",
    "        connection_string = f\"{dialect}+{driver}://{username}:{password}@{host}:{port}/{database}\"\n",
    "        self.engine = sa.create_engine(connection_string)\n",
    "\n",
    "    def insert_dataframe(self, df: pd.DataFrame, table_name: str) -> None:\n",
    "        \"\"\"Inserts a dataframe into a table\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): dataframe to insert\n",
    "            table_name (str): table name\n",
    "        \"\"\"\n",
    "        df.to_sql(table_name, self.engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    def query_to_df(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Retrieves a dataframe from a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): query to perform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: daframe with the results of the query.\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df\n",
    "\n",
    "    def check_connection(self) -> bool:\n",
    "        \"\"\"Checks if the connection is working\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the connection is working, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.engine.connect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataframe to upload to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_test.index.to_list()\n",
    "\n",
    "# Create a dataframe with the data to store\n",
    "df_article_prediction = pd.DataFrame({\n",
    "    \"fecha\": dates,\n",
    "    \"cantidad\": predictions,\n",
    "    \"articulo\": [PRODUCT_ID] * len(predictions),                                     # repeat the article for each date\n",
    "    \"familia\": [PRODUCT_FAMILY] * len(predictions),                                  # repeat the family for each date\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the dataframe to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables to hide credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "USER = os.getenv(\"EV_user\")\n",
    "PASSWORD = os.getenv(\"EV_password\")\n",
    "NAME = \"david97p\"\n",
    "table_name = f\"Materials_Prediction_Group_{NAME}\"\n",
    "\n",
    "# Connect to the database\n",
    "db = DatabaseConnection(USER, PASSWORD)\n",
    "db.insert_dataframe(df_article_prediction, table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
