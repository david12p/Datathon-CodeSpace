{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon III - MLOps\n",
    "\n",
    "In previous lessonss we've already preprocess the data and created a dataset for training. In this lesson we will:\n",
    "\n",
    "- üëâ Split the data into: Train / Test / Validation (0.5 p)\n",
    "- üëâ Train a Regression Model Pipeline (1 p)\n",
    "- üëâ Evaluate the model (1 p)\n",
    "- üëâ Log the model and the metrics using MLflow (1 p)\n",
    "- üëâ Register the model in MLflow (1 p)\n",
    "- üëâ Deploy the model to a REST API (1 p)\n",
    "- üëâ Make predictions using the REST API (1 p)\n",
    "- üëâ Upload the predictions to the database (0.5 p)\n",
    "\n",
    "Also the following activities will be evaluated:\n",
    "\n",
    "- üëâ Code legibility (0.5 p)\n",
    "- üëâ Notebook documentation: titles, subtitles, text explanation, etc. (0.5 p)\n",
    "- üëâ Creating a GIT repository with the code:\n",
    "  - üëâ Adding a complete README.md file (0.5 p)\n",
    "  - üëâ Adding a .gitignore file and a requirements.txt file (0.5 p)\n",
    "  - üëâ Using Branches for development (1 p)\n",
    "\n",
    "Bonus points:\n",
    "\n",
    "- ‚úÖ Any other activity that the student considers important and reflected in code (ie.: GridSearch)\n",
    "- ‚úÖ Trying with other articles\n",
    "\n",
    "Following activities will be negatively evaluated:\n",
    "\n",
    "- ‚ùå Pushing Data to the repository (it's a bad practice)\n",
    "- ‚ùå Pushing passwords or sensitive information to the repository (It's a VERY bad practice)\n",
    "- ‚ùå Pushing files generated by MLFlow like `mlruns` (it's a bad practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid unnecessary warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First step always is data loading. CSV are provided and can be replicated from the previows lesson. CSVs contains data for the PRODUCT_ID 3960.\n",
    "\n",
    "- üëâ Load train and test datasets.\n",
    "- üëâ Set \"fecha_venta\" as index column.\n",
    "- üëâ Sort the data by \"fecha_venta\" column.\n",
    "- üëâ Show the first 5 rows of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/itemSalesTrain.csv\"                          #This file only includes data of the article 3960\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)             \n",
    "df_train = df_train.set_index(\"fecha_venta\").sort_index()            #Set fecha_venta as index and sort by fecha_venta column\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"data/itemSalesTest.csv\"\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "df_test = df_test.set_index(\"fecha_venta\").sort_index()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n",
    "\n",
    "We will split the data into train and test sets. We will use the train set to train the model and the test set to evaluate it.\n",
    "\n",
    "\n",
    "- üëâ Select the feature columns and the label columns\n",
    "- üëâ Split the data into train and validation sets using a 80/20 or 90/10 ratio.\n",
    "- üí° Remember how train/val split should be made in time series problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the product id and family (only for logging purposes, do not use it for filtering)\n",
    "PRODUCT_ID = 3960\n",
    "PRODUCT_FAMILY = \"BOLLERIA\"\n",
    "\n",
    "df_test.columns     # To see what dependent variable we want to use (var1(t)). Then, we set the label and the features\n",
    "\n",
    "LABEL_COLUMN = \"var1(t)\"\n",
    "\n",
    "FEATURES_COLUMN = [\n",
    "    'var1(t-7 day)',\n",
    "    'var1(t-6 day)', \n",
    "    'var1(t-5 day)', \n",
    "    'var1(t-4 day)',\n",
    "    'var1(t-3 day)', \n",
    "    'var1(t-2 day)', \n",
    "    'var1(t-1 day)', \n",
    "    'var1(t-5 week)',\n",
    "    'var1(t-4 week)', \n",
    "    'var1(t-3 week)', \n",
    "    'var1(t-2 week)', \n",
    "    'var1(t-1 week)',\n",
    "    'var1(t)', \n",
    "    'tavg_w', \n",
    "    'tmin_w', \n",
    "    'tmax_w', \n",
    "    'isfestivo', \n",
    "    'day',\n",
    "    'dayofweek', \n",
    "    'month', \n",
    "    'year', \n",
    "    'weekday'\n",
    "]\n",
    "\n",
    "features = df_train[FEATURES_COLUMN]\n",
    "labels = df_train[LABEL_COLUMN]\n",
    "\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "train_size = int(len(df_train) * (1 - VAL_SPLIT))       # 90% of the data\n",
    "\n",
    "X_train = features[:train_size]\n",
    "y_train = labels[:train_size]\n",
    "X_val = features[train_size:]\n",
    "y_val = labels[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start MLFlow Server\n",
    "\n",
    "- üëâ Launch a local MLFlow server\n",
    "- üëâ Connect to local MLFlow server\n",
    "- üëâ Set the desired experiment\n",
    "- üëâ Enable MLFlow autologing for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow==2.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the mlflow server hosted locally and setting the url in a variable\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)        #To launch the server, we open the terminal and write \"mlflow ui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we create a new experiment (I did it on the webpage, it¬¥s called \"Sales Forecasting\"), we have to connect to the experiment.\n",
    "MLFLOW_EXPERIMENT_NAME = \"Sales Forecasting\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)       # Set what the experiment is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the autlog sklearn\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "\n",
    "The next section is to train and evaluate the model. We will use a pipeline to preprocess the data and train the model.\n",
    "\n",
    "- üëâ Create a Sklearn Pipeline:\n",
    "  - üëâ Preprocessing: StandardScaler or MinMaxScaler\n",
    "  - üëâ Model: LinearRegression, RandomForestRegressor, etc.\n",
    "- üëâ Start a run in MLFlow\n",
    "- üëâ Train the model using the train dataset\n",
    "- üëâ Add convenient tags for PRODUCT_ID and FAMILY_ID\n",
    "- üëâ Evaluate the model\n",
    "- üí° Remember this is a regression problem\n",
    "- üí° Autolog will automatically log metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "model = LinearRegression()\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "with mlflow.start_run() as run:                             # Start a run with these information:\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    mlflow.set_tag(\"product_id\", PRODUCT_ID)                # Add tags in columns\n",
    "    mlflow.set_tag(\"product_family\", PRODUCT_FAMILY)\n",
    "\n",
    "    predictions = pipe.predict(X_val)\n",
    "    mae = metrics.mean_absolute_error(y_val, predictions)   # Add mae, mse and r2 metrics\n",
    "    mse = metrics.mean_squared_error(y_val, predictions)\n",
    "    r2 = metrics.r2_score(y_val, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "Promote the model to Model Registry. For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Sales Forecasting](data/model1_mlflow_production.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the Model\n",
    "\n",
    "We can assign a tag to the model to indicate that it is ready for production. This way all versions (v1, v2...) of the model will have the same tag. So we can deploy the model by selecting the (same) tag instead of a specific (different) version.\n",
    "\n",
    "For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Sales Forecasting Tag \"production\"](data/mlflow_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "In a terminal run the following command to deploy the model:\n",
    "\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "mlflow models serve -m models:/<model_name>@production -p 5001 --env-manager local\n",
    "```\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```bash\n",
    "[INFO] Starting gunicorn 21.2.0\n",
    "[INFO] Listening at: http://127.0.0.1:5001 (236041)\n",
    "[INFO] Using worker: sync\n",
    "[INFO] Booting worker with pid: 236048\n",
    "```\n",
    "\n",
    "It means it's working correctly üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make requests to the model\n",
    "\n",
    "The model is now deployed and ready to receive requests. We will make a request to the model using the test set.\n",
    "\n",
    "- üëâ prepare the test set to be sent as JSON\n",
    "- üëâ make a POST request to the model\n",
    "- üëâ get the predictions from the response and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "URL = \"http://localhost:5001/invocations\"\n",
    "#URL = \"http://localhost:5001/health\"            \n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": df_test.columns.tolist(),\n",
    "        \"index\": df_test.index.to_list(),\n",
    "        \"data\": df_test.values.tolist()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(URL)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(URL, headers=headers, data=json.dumps(data))\n",
    "predictions = response.json()[\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Results to Database\n",
    "\n",
    "We push the results to the database so we can visualize them using other tools like Tableau, PowerBI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpfull class used to connect to the database and push dataframes\n",
    "\n",
    "import sqlalchemy as sa\n",
    "\n",
    "class DatabaseConnection:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username: str,\n",
    "        password: str,\n",
    "        dialect: str = \"mysql\",\n",
    "        driver: str = \"pymysql\",\n",
    "        host: str = \"database-1.cxlpff3hacbu.eu-west-3.rds.amazonaws.com\",\n",
    "        port: int = 3306,\n",
    "        database: str = \"sandbox\",\n",
    "    ) -> None:\n",
    "        \"\"\"Creates a connection to a database\n",
    "\n",
    "        Args:\n",
    "            username (str): username\n",
    "            password (str): password\n",
    "            dialect (str, optional): dialect. Defaults to \"mysql\".\n",
    "            driver (str, optional): driver. Defaults to \"pymysql\".\n",
    "            host (str, optional): host. Defaults to \"database-1.crek3tiqyj7r.eu-west-3.rds.amazonaws.com\".\n",
    "            port (int, optional): port. Defaults to 3306.\n",
    "            database (str, optional): database. Defaults to \"classicmodels\".\n",
    "        \"\"\"\n",
    "        connection_string = f\"{dialect}+{driver}://{username}:{password}@{host}:{port}/{database}\"\n",
    "        self.engine = sa.create_engine(connection_string)\n",
    "\n",
    "    def insert_dataframe(self, df: pd.DataFrame, table_name: str) -> None:\n",
    "        \"\"\"Inserts a dataframe into a table\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): dataframe to insert\n",
    "            table_name (str): table name\n",
    "        \"\"\"\n",
    "        df.to_sql(table_name, self.engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    def query_to_df(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Retrieves a dataframe from a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): query to perform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: daframe with the results of the query.\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df\n",
    "\n",
    "    def check_connection(self) -> bool:\n",
    "        \"\"\"Checks if the connection is working\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the connection is working, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.engine.connect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataframe to upload to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_test.index.to_list()\n",
    "\n",
    "# Create a dataframe with the data to store\n",
    "df_article_prediction = pd.DataFrame({\n",
    "    \"fecha\": dates,\n",
    "    \"cantidad\": predictions,\n",
    "    \"articulo\": [PRODUCT_ID] * len(predictions),                                     # repeat the article for each date\n",
    "    \"familia\": [PRODUCT_FAMILY] * len(predictions),                                  # repeat the family for each date\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the dataframe to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables to hide credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "USER = os.getenv(\"EV_user\")\n",
    "PASSWORD = os.getenv(\"EV_password\")\n",
    "NAME = \"david97p\"\n",
    "table_name = f\"Materials_Prediction_Group_{NAME}\"\n",
    "\n",
    "# Connect to the database\n",
    "db = DatabaseConnection(USER, PASSWORD)\n",
    "db.insert_dataframe(df_article_prediction, table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
