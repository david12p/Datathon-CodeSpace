{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon III - MLOps\n",
    "\n",
    "In previous lessonss we've already preprocess the data and created a dataset for training. In this lesson we will:\n",
    "\n",
    "- 👉 Split the data into: Train / Test / Validation (0.5 p)\n",
    "- 👉 Train a Regression Model Pipeline (1 p)\n",
    "- 👉 Evaluate the model (1 p)\n",
    "- 👉 Log the model and the metrics using MLflow (1 p)\n",
    "- 👉 Register the model in MLflow (1 p)\n",
    "- 👉 Deploy the model to a REST API (1 p)\n",
    "- 👉 Make predictions using the REST API (1 p)\n",
    "- 👉 Upload the predictions to the database (0.5 p)\n",
    "\n",
    "Also the following activities will be evaluated:\n",
    "\n",
    "- 👉 Code legibility (0.5 p)\n",
    "- 👉 Notebook documentation: titles, subtitles, text explanation, etc. (0.5 p)\n",
    "- 👉 Creating a GIT repository with the code:\n",
    "  - 👉 Adding a complete README.md file (0.5 p)\n",
    "  - 👉 Adding a .gitignore file and a requirements.txt file (0.5 p)\n",
    "  - 👉 Using Branches for development (1 p)\n",
    "\n",
    "Bonus points:\n",
    "\n",
    "- ✅ Any other activity that the student considers important and reflected in code (ie.: GridSearch)\n",
    "- ✅ Trying with other articles\n",
    "\n",
    "Following activities will be negatively evaluated:\n",
    "\n",
    "- ❌ Pushing Data to the repository (it's a bad practice)\n",
    "- ❌ Pushing passwords or sensitive information to the repository (It's a VERY bad practice)\n",
    "- ❌ Pushing files generated by MLFlow like `mlruns` (it's a bad practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid unnecessary warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "First step always is data loading. CSV are provided and can be replicated from the previows lesson. CSVs contains data for the PRODUCT_ID 3960.\n",
    "\n",
    "- 👉 Load train and test datasets.\n",
    "- 👉 Set \"fecha_venta\" as index column.\n",
    "- 👉 Sort the data by \"fecha_venta\" column.\n",
    "- 👉 Show the first 5 rows of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 👇 Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets\n",
    "\n",
    "We will split the data into train and test sets. We will use the train set to train the model and the test set to evaluate it.\n",
    "\n",
    "\n",
    "- 👉 Select the feature columns and the label columns\n",
    "- 👉 Split the data into train and validation sets using a 80/20 or 90/10 ratio.\n",
    "- 💡 Remember how train/val split should be made in time series problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the product id and family (only for logging purposes, do not use it for filtering)\n",
    "PRODUCT_ID = 3960\n",
    "PRODUCT_FAMILY = \"BOLLERIA\"\n",
    "\n",
    "# 👇 Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start MLFlow Server\n",
    "\n",
    "- 👉 Launch a local MLFlow server\n",
    "- 👉 Connect to local MLFlow server\n",
    "- 👉 Set the desired experiment\n",
    "- 👉 Enable MLFlow autologing for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# 👇 Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "\n",
    "The next section is to train and evaluate the model. We will use a pipeline to preprocess the data and train the model.\n",
    "\n",
    "- 👉 Create a Sklearn Pipeline:\n",
    "  - 👉 Preprocessing: StandardScaler or MinMaxScaler\n",
    "  - 👉 Model: LinearRegression, RandomForestRegressor, etc.\n",
    "- 👉 Start a run in MLFlow\n",
    "- 👉 Train the model using the train dataset\n",
    "- 👉 Add convenient tags for PRODUCT_ID and FAMILY_ID\n",
    "- 👉 Evaluate the model\n",
    "- 💡 Remember this is a regression problem\n",
    "- 💡 Autolog will automatically log metrics and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# 👇 Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "\n",
    "Promote the model to Model Registry. For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👇 Code Goes Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the Model\n",
    "\n",
    "We can assign a tag to the model to indicate that it is ready for production. This way all versions (v1, v2...) of the model will have the same tag. So we can deploy the model by selecting the (same) tag instead of a specific (different) version.\n",
    "\n",
    "For this section you can choose between using the MLflow UI or using code snipets. If you choose the UI you should provide screenshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ALIAS = \"production\"  # model will promote to this stage\n",
    "\n",
    "\n",
    "# 👇 Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "In a terminal run the following command to deploy the model:\n",
    "\n",
    "```bash\n",
    "export MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "mlflow models serve -m models:/<model_name>@production -p 5001 --env-manager local\n",
    "```\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```bash\n",
    "[INFO] Starting gunicorn 21.2.0\n",
    "[INFO] Listening at: http://127.0.0.1:5001 (236041)\n",
    "[INFO] Using worker: sync\n",
    "[INFO] Booting worker with pid: 236048\n",
    "```\n",
    "\n",
    "It means it's working correctly 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make requests to the model\n",
    "\n",
    "The model is now deployed and ready to receive requests. We will make a request to the model using the test set.\n",
    "\n",
    "- 👉 prepare the test set to be sent as JSON\n",
    "- 👉 make a POST request to the model\n",
    "- 👉 get the predictions from the response and show them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# 👇 Code Goes Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Results to Database\n",
    "\n",
    "We push the results to the database so we can visualize them using other tools like Tableau, PowerBI, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpfull class used to connect to the database and push dataframes\n",
    "\n",
    "import sqlalchemy as sa\n",
    "\n",
    "\n",
    "class DatabaseConnection:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        username: str,\n",
    "        password: str,\n",
    "        dialect: str = \"mysql\",\n",
    "        driver: str = \"pymysql\",\n",
    "        host: str = \"database-1.cxlpff3hacbu.eu-west-3.rds.amazonaws.com\",\n",
    "        port: int = 3306,\n",
    "        database: str = \"sandbox\",\n",
    "    ) -> None:\n",
    "        \"\"\"Creates a connection to a database\n",
    "\n",
    "        Args:\n",
    "            username (str): username\n",
    "            password (str): password\n",
    "            dialect (str, optional): dialect. Defaults to \"mysql\".\n",
    "            driver (str, optional): driver. Defaults to \"pymysql\".\n",
    "            host (str, optional): host. Defaults to \"database-1.crek3tiqyj7r.eu-west-3.rds.amazonaws.com\".\n",
    "            port (int, optional): port. Defaults to 3306.\n",
    "            database (str, optional): database. Defaults to \"classicmodels\".\n",
    "        \"\"\"\n",
    "        connection_string = f\"{dialect}+{driver}://{username}:{password}@{host}:{port}/{database}\"\n",
    "        self.engine = sa.create_engine(connection_string)\n",
    "\n",
    "    def insert_dataframe(self, df: pd.DataFrame, table_name: str) -> None:\n",
    "        \"\"\"Inserts a dataframe into a table\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): dataframe to insert\n",
    "            table_name (str): table name\n",
    "        \"\"\"\n",
    "        df.to_sql(table_name, self.engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    def query_to_df(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Retrieves a dataframe from a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): query to perform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: daframe with the results of the query.\n",
    "        \"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df\n",
    "\n",
    "    def check_connection(self) -> bool:\n",
    "        \"\"\"Checks if the connection is working\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the connection is working, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.engine.connect()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataframe to upload to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👇 Fill this schema with the relevant data \n",
    "\n",
    "# Create a dataframe with the data to store\n",
    "df_article_prediction = pd.DataFrame({\n",
    "    \"fecha\": [],\n",
    "    \"cantidad\": [],\n",
    "    \"articulo\": [],  # repeat the article for each date\n",
    "    \"familia\": [],  # repeat the family for each date\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the dataframe to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "USER = 'usuario1'\n",
    "PASSWORD = 'C0d35p4ce.'\n",
    "NAME = \"\"\n",
    "table_name = f\"Materials_Prediction_Group_{NAME}\"\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "db = DatabaseConnection(USER, PASSWORD)\n",
    "db.insert_dataframe(df_article_prediction, table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
